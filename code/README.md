## Code

This folder will contain jupyter notebooks as well as other python scripts that I used to construct and learn. Some files I will label with 
"trial #" please note that these files may not execute correctly. That is because there might be errors that were integral to the learning process so I documented them. I will post notes about each file here as I upload them. For the data, I was able to get access to the Wharton School of Business database through my university. Other solutions would be using an API such as Alpha Vantage to grab more current data. I plan on using Alpha Vantage to get data to test my model. 


## ML_trial3_withh_all_data

In this notebook, I simplify the model to have 2 layers of 15 neurons. These numbers were chosen at random but the preformance of the model wasn't that bad. 
The model's training loss eventaually went below 1, but the validation loss indicated an overfit model or a vanishing gradient problem. To fix these I will need to tweak the model a bit more. 


## Trial 4

For trial 4 I created several models that utilized a few methods I researched that are supposed to help with overfitting and/or vanishing gradient problems. I made limited progress. With this model I was able to get the model to stop overfitting but the vanishing gradient is still an issue. I can tell this is the problem because when looking at the graph for this model, you can see the training loss (labeled 'Loss') goes down to close to 0 (good). But the validation loss (labeled 'val_loss') decreases and then stops decreasing. This likely means that the model has stopped learning. If the model was overfitting, the val_loss line would start to increase again.  Below is an image of the graph and the code is listed in the folder.

![Trial4 image]( /picture/trial4_graph.PNG)


### Tweak parameters

For these next iterations of my model, I choose to simply tweak the parameters of the model I had created. These parameters are as follows: Number of layers, number of neurons per layer, activation function, using bias neurons, using kernal initializers, and optimization functions. Not only did I spend time tweaking these parameters but I sought help through the machine learning community on [stackoverflow](http://stackoverflow.com). There were a limited number of replies on my post regarding optimization of a model but I was able to take what was said and change a few things in order to get a model that seemed to work differently. Not necessarilly better, but something more understandable for a beginner. I changed the activation function to the popular ReLu function, and used it for all layers (except the output). Then used the 'adam' optimizer for backpropagation and I also changed the model dimensions. That is I changed it to be two layers with two neurons each and then an output layer with one neuron. I did this because I changed the input data set to only include price and volume (two features). This is because before I was dealing with 26 features. I thought that if I could get a model to work with data that was more simple, then I could try and work with more complex data afterwards. 
The results were mixed and I tried about a dozen different models but I still have two major problems. The first being overfitting. This is common and there are many reasons that a model would be overfit. Such as a model that is too complex, data that is not correctly normalized, or the absence of regularizers on the weights. The next problem is that my model doesn't reach the desired range for loss and validation loss. However, the validation loss doesn't increase which means that it is still technically learning but at a much slower rate. This is called the vanishing gradient problem. As models get larger and more complex, the gradients on the lower levels get smaller and smaller eventually ending up around zero. This problem I am still quite new to and will be the subject of further research as the project goes on.

### Polynomial regression

I learn by getting my hands dirty and by trying examples and experimenting with the libraries and code available. This is why I did minimum research before jumping in and trying to build a model. It was probably a little naive of me to think that I would be able to produce a model with even remotely good results and that of course didn't happen. Thus, I went back to reading and research and found another avenue to experiment with: Polynomial regression. So far my trials have consisted of linear regression and sequential neural networks. However, my data is not linear in nature. There are many variables and relationships that affect the price of a stock. This seems to indicate that polynomial regression (PR) would be the perfect tool.

### Recurrent Neural Network (RNN)

After much more reading on the explicit problem of predicting stock prices, I learned that lots of people have had succues (to a degree) using RNNs. These models use the output of a layer as an additional input to the previous layer, adding a feedback loop in the model design. All the previous models I have tried are considered feedforward because they do not loop back. Adding this loop back can allow of the model to learn better from historical data and have "memory." This is extremly important when talking about stock data because for technical analysis predicitons, this is basically the definition: using historical technical data to make a prediction about the next price. I found a good tutorial about RNNs and they just so happen to be solving the same problem. The file that is labeled ["ece_448_final_fall_algorithm"](/code/ece_448_final_fall_algorithm.ipynb) is our recurrent neural network model. This is just the one that [Ahmed](towardsdatascience.com/getting-rich-quick-with-machine-learning-and-stock-market-predictions-696802da94fe) used in his model, but we decided to see if it was accurate in this scenario. Turns out this model is pretty accurate at predicting trends but on an indivdual level there is still a decent amount of error. Even if the average is 10%, sometimes when predicting a price like Amazon around $3000 you can get predictions of about $1000 or less. 

### Solution setup as of Nov. 2020 
 ["ece_448_final_fall_algorithm"](/code/ece_448_final_fall_algorithm.ipynb)
 
To come up with a solution we began looking at various different machine learning models. These models included polynomial regression, classification, and neural networks. We began with polynomial regression because stock data is rarely if ever linear. There are many different variables that go into predicting stock prices such as price, volume and technical indicators,  so polynomial regression seemed like a logical first step. However, when we implemented a polynomial regression model with several different technical indicators, the calculations required by the computer were too great. We implemented this polynomial regression algorithm in a jupyter notebook using tensorflow and keras and when we ran the simulation, the computer was unable to finish. We took this as a signal that polynomial regression was most likely not going to suit our needs although if we adjusted the input data we might have been able to get a finished model. After the polynomial regression model we researched classification models. Many people use these models because they are easy to understand and implement. Some DIY algorithm traders have even had limited success in programming a trading algorithm using a classification model. We are still in the process of researching this model and constructing a suitable test for it which we will discuss later in the future work section. 

While researching classification models, we found that an increasing number of people were turning to neural networks to do stock predictions. This led us to our first neural network: the feedforward model. In a feedforward model, data is fed into the neural network and a prediction is given as the output. In between the input and output are layers of “neurons” which are essentially just step functions for the data to be fed into. When the data is sent into a neuron the value of that piece of data is evaluated in a step function. If the data is greater than the threshold value, the neuron outputs a 1, if not then the output is a 0. There are also complicated weights and other functions associated with neural network construction but for simplicity this description is suitable. When building a neural network one must decide how many layers in between the input and output there should be and how many neurons in each layer there needs to be. There isn’t a mathematical way to determine these “hyperparameters” but there are guidelines that people follow that tend to give the best solutions. A person cannot just pick random values because it may take a long time to narrow down the search for the best model. Furthermore, values that do not produce an accurate model can lead to overfitting and underfitting. When constructing our feedforward model, we decided to have only two layers between the input and output. Through experimentation, we found that any number greater than that would lead to a vanishing gradient problem and any number smaller would lead to overfitting/underfitting. As for the number of neurons, it is widely accepted that one must have as many neurons as input features into the model plus one extra as a biased neuron (A bias neuron is a neuron that always outputs a 1 no matter the input). Thus, we landed on 28 neurons in each layer (This is before we reduced the dimensionality of the model). 

The results we got from this model were not great. We learned that the model was still impacted by the vanishing gradient problem. To fix this error, one must reduce the complexity of the model. To accomplish this we tried two techniques. The first was to reduce the number of neurons in each layer. This would give the data a more direct path to the output. The second was to reduce the dimensionality of the input data. Because we had 27 parameters that we were running through the model, it is possible that the model was overwhelmed by the amount of data and couldn't handle it. To solve this issue, we reduced the number of features down from 27 to 16. These had little impact on the results that we saw and we still had a vanishing gradient and/or an overfitting problem. This led us to our final model choice: the recurrent neural network (RNN). 

The RNN works very much like a feedforward neural network but there is a feedback loop embedded in the RNN which allows the outputs of RNN or neurons in the RNN to be fed back into the input allowing for the model to make predictions based on historical data that was fed into the system. There are a few different models of RNNs but we chose the long short-term memory model or LSTM model. This is because this model allows the historical data to bypass a lot of the mathematical functions of a neuron and speeds up the training process. Another reason we landed on this model is that there seems to be a good number of DIY algorithm traders that have built their own models using this technique which we drew upon in order to experiment with our model. The LSTM model is good because it allows us to input consecutive historical data and make a prediction about the future which is exactly what we wanted to do. Below you can see a single LSTM neuron as an example.

![LSTM model picture](picture/lstm.png)
As you can see there is a pathway for data at the top that allows for data to be passed back into the neuron and skip the mathematical functions below.

Our results are very promising. To begin one must understand the different types of models that an RNN can take on. These are sequence-to-sequence, sequence-to-vector and vector-to-sequence. Sequence refers to a dataset that is continuous and consecutive and a vector is a single value. In our model a sequence will consist of X amount of previous days worth of data points and our vector will be the next day's closing price. Our model is constructed in the following way: 
![model](/picture/our_model.PNG)

As you can see from the above image, we have an input layer that takes in a shape of (points,num_of_cols). We did this to allow for a more robust algorithm. This allows the user to customize the model by customizing their own dataset. We also have a LSTM layer of 50 neurons and a dense layer of 64 neurons before we reach our output layer of one neuron. After the LSTM layer there is a line that says “dropout.” This means that at the rate that is specified in the function (0.2) the model will turn off a certain number of neurons in that layer (LSTM layer). This is because when we learn as humans we do not use all of our neurons and we often use heuristics to speed the process up. This simulates that in a neural network. Additionally, you can see that we use a sigmoid function as our activation function for the dense layer but there is no activation function for the LSTM layer. THis is because the LSTM layer by default has its own activation function built in. We do not have access to change that parameter in this model. Also, you can see that we use the “Adam” optimization function and not the standard stochastic gradient descent (SGD). When dealing with RNNs, SGD takes much longer and is not as accurate as the Adam function. The optimization function is the function that evaluates the model and its layers/neurons to determine where the error in the output has occurred and it adjusts the values of the weights accordingly. This is done through a process called backpropagation. We won’t get too deep into this because it is very mathematically dense, but essentially these optimization functions determine what percent of the overall error at the output came from each neuron and layer and adjusts the values of the weights and functions at those layers to try and eliminate that error. 

Finally, after training our model we can run predictions through it to see how accurate we were. When training the RNN we had to keep in mind that we wanted to feed the model X days worth of consecutive data points. This means we could not shuffle and randomize the data points in our dataset. This is how you would normally train a feedforward model but with our RNN model it would underfit the model because it would have a significant increase in difficulty determining the relationship between consecutive data points and the output when it was trained on random data points. Below are our final results from training.



## sources
Ahmed, Yacoub. “Getting Rich Quick with Machine Learning and Stock Market Predictions.” Medium, Towards Data Science, 12 Jan. 2020, towardsdatascience.com/getting-rich-quick-with-machine-learning-and-stock-market-predictions-696802da94fe. 

